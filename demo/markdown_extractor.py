import gradio as gr
import os
import re
import json
from typing import Dict, List, Tuple
from dotenv import load_dotenv
import dashscope
from dashscope import Generation
import pdfplumber

load_dotenv()
dashscope.api_key = os.getenv('DASHSCOPE_API_KEY')

def extract_markdown_content(file):
    if file is None:
        return "", None
    
    try:
        file_path = file.name
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if file_ext == '.pdf':
            content = pdf_to_markdown(file_path)
        elif file_ext in ['.md', '.markdown']:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        else:
            return "é”™è¯¯: è¯·ä¸Šä¼ .mdã€.markdownæˆ–.pdfæ ¼å¼çš„æ–‡ä»¶", None
        
        analysis = analyze_resume_with_llm(content)
        analysis_result = format_llm_analysis_result(analysis)
        
        return analysis_result, analysis
    
    except UnicodeDecodeError:
        return "é”™è¯¯: æ–‡ä»¶ç¼–ç ä¸æ˜¯UTF-8ï¼Œè¯·ç¡®ä¿æ–‡ä»¶ä½¿ç”¨UTF-8ç¼–ç ", None
    except Exception as e:
        return f"é”™è¯¯: {str(e)}", None

def clear_content():
    return "", None

def pdf_to_markdown(pdf_path: str) -> str:
    try:
        markdown_content = []
        
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, 1):
                text = page.extract_text()
                
                if text:
                    lines = text.split('\n')
                    
                    for line in lines:
                        line = line.strip()
                        if line:
                            markdown_content.append(line)
                    
                    if page_num < len(pdf.pages):
                        markdown_content.append('')
        
        return '\n'.join(markdown_content)
    
    except Exception as e:
        raise Exception(f"PDFè½¬æ¢å¤±è´¥: {str(e)}")

def analyze_resume_with_llm(content: str) -> Dict[str, any]:
    prompt = f"""è¯·åˆ†æä»¥ä¸‹ç®€å†å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ã€‚ç®€å†å†…å®¹å¦‚ä¸‹ï¼š

{content}

è¯·æå–ä»¥ä¸‹ä¿¡æ¯å¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "name": "å§“å",
    "contact": {{
        "phone": "æ‰‹æœºå·",
        "email": "é‚®ç®±",
        "location": "æ‰€åœ¨åœ°"
    }},
    "target_position": "æ±‚èŒæ„å‘/åº”è˜å²—ä½",
    "skills": ["æŠ€èƒ½1", "æŠ€èƒ½2", "æŠ€èƒ½3"],
    "experience_years": "å·¥ä½œå¹´é™",
    "education": "å­¦å†ä¿¡æ¯",
    "summary": "ä¸ªäººç®€ä»‹/è‡ªæˆ‘è¯„ä»·",
    "key_highlights": ["äº®ç‚¹1", "äº®ç‚¹2", "äº®ç‚¹3"]
}}

æ³¨æ„äº‹é¡¹ï¼š
1. å¦‚æœæŸä¸ªä¿¡æ¯åœ¨ç®€å†ä¸­æ‰¾ä¸åˆ°ï¼Œå¯¹åº”å­—æ®µè¿”å›ç©ºå­—ç¬¦ä¸²æˆ–ç©ºæ•°ç»„
2. skillså­—æ®µæå–æ‰€æœ‰æŠ€æœ¯æŠ€èƒ½ã€ç¼–ç¨‹è¯­è¨€ã€æ¡†æ¶ç­‰
3. key_highlightsæå–å€™é€‰äººçš„æ ¸å¿ƒä¼˜åŠ¿æˆ–äº®ç‚¹
4. åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—è¯´æ˜
"""

    try:
        response = Generation.call(
            model='qwen-turbo',
            prompt=prompt,
            result_format='message'
        )
        
        if response.status_code == 200:
            result_text = response.output.choices[0]['message']['content']
            
            json_match = re.search(r'\{[\s\S]*\}', result_text)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                if not isinstance(result, dict):
                    raise ValueError("è¿”å›çš„ä¸æ˜¯å­—å…¸æ ¼å¼")
                
                required_keys = ['name', 'contact', 'target_position', 'skills', 'experience_years', 'education', 'summary', 'key_highlights']
                for key in required_keys:
                    if key not in result:
                        result[key] = "" if key != 'skills' and key != 'key_highlights' else []
                
                if not isinstance(result['contact'], dict):
                    result['contact'] = {'phone': '', 'email': '', 'location': ''}
                
                contact_keys = ['phone', 'email', 'location']
                for key in contact_keys:
                    if key not in result['contact']:
                        result['contact'][key] = ''
                
                return result
            else:
                raise ValueError("æ— æ³•ä»å“åº”ä¸­æå–JSON")
        else:
            raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.message}")
    
    except Exception as e:
        print(f"å¤§æ¨¡å‹åˆ†æå‡ºé”™: {str(e)}")
        return analyze_resume(content)

def format_llm_analysis_result(analysis: Dict[str, any]) -> str:
    output = []
    
    output.append("## ğŸ“‹ ç®€å†åˆ†æç»“æœï¼ˆAIå¢å¼ºç‰ˆï¼‰\n")
    
    if analysis["name"]:
        output.append(f"**å§“å**: {analysis['name']}\n")
    
    output.append("### ğŸ“ è”ç³»æ–¹å¼")
    if analysis["contact"]["phone"]:
        output.append(f"- ç”µè¯: {analysis['contact']['phone']}")
    if analysis["contact"]["email"]:
        output.append(f"- é‚®ç®±: {analysis['contact']['email']}")
    if analysis["contact"]["location"]:
        output.append(f"- åœ°å€: {analysis['contact']['location']}")
    
    if analysis["target_position"]:
        output.append(f"\n### ğŸ¯ æ±‚èŒæ„å‘")
        output.append(f"- åº”è˜å²—ä½: {analysis['target_position']}")
    
    if analysis["experience_years"]:
        output.append(f"- å·¥ä½œå¹´é™: {analysis['experience_years']}")
    
    if analysis["education"]:
        output.append(f"\n### ğŸ“ å­¦å†ä¿¡æ¯")
        output.append(f"- {analysis['education']}")
    
    if analysis["skills"]:
        output.append(f"\n### ğŸ’¡ ä¸“ä¸šæŠ€èƒ½")
        for skill in analysis["skills"]:
            output.append(f"- {skill}")
    
    if analysis["key_highlights"]:
        output.append(f"\n### â­ æ ¸å¿ƒäº®ç‚¹")
        for highlight in analysis["key_highlights"]:
            output.append(f"- {highlight}")
    
    if analysis["summary"]:
        output.append(f"\n### ğŸ‘¤ ä¸ªäººç®€ä»‹")
        output.append(analysis['summary'])
    
    return '\n'.join(output)

def load_interview_questions(questions_file: str = 'interview_questions.json') -> Dict:
    try:
        with open(questions_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        return {"categories": {}, "difficulty_levels": {}}

def match_category_from_position(target_position: str, questions_db: Dict) -> List[str]:
    position_lower = target_position.lower()
    matched_categories = []
    
    category_keywords = {
        'frontend': ['å‰ç«¯', 'front-end', 'webå‰ç«¯', 'ui', 'vue', 'react', 'angular', 'javascript', 'typescript'],
        'backend': ['åç«¯', 'back-end', 'æœåŠ¡ç«¯', 'server', 'api', 'java', 'python', 'go', 'node.js', 'spring'],
        'python': ['python', 'py', 'çˆ¬è™«', 'æ•°æ®åˆ†æ'],
        'ai_ml': ['ai', 'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'ml', 'æ·±åº¦å­¦ä¹ ', 'deep learning', 'nlp', 'ç®—æ³•'],
        'database': ['æ•°æ®åº“', 'database', 'db', 'mysql', 'postgresql', 'mongodb', 'redis'],
        'devops': ['devops', 'è¿ç»´', 'docker', 'kubernetes', 'k8s', 'ci/cd', 'éƒ¨ç½²']
    }
    
    for category, keywords in category_keywords.items():
        if any(keyword in position_lower for keyword in keywords):
            matched_categories.append(category)
    
    return matched_categories if matched_categories else ['frontend', 'backend']

def match_category_from_skills(skills: List[str], questions_db: Dict) -> List[str]:
    matched_categories = []
    
    skill_category_mapping = {
        'frontend': ['react', 'vue', 'angular', 'javascript', 'typescript', 'html', 'css', 'webpack', 'vite'],
        'backend': ['java', 'spring', 'node.js', 'express', 'django', 'flask', 'go', 'rust', 'php'],
        'python': ['python', 'pandas', 'numpy', 'scikit-learn', 'django', 'flask'],
        'ai_ml': ['tensorflow', 'pytorch', 'keras', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'nlp', 'ç®—æ³•', 'æ¨¡å‹'],
        'database': ['mysql', 'postgresql', 'mongodb', 'redis', 'elasticsearch', 'sql', 'nosql'],
        'devops': ['docker', 'kubernetes', 'k8s', 'git', 'ci/cd', 'linux', 'aws', 'azure', 'gcp']
    }
    
    for category, category_skills in skill_category_mapping.items():
        for skill in skills:
            skill_lower = skill.lower()
            if any(cs in skill_lower for cs in category_skills):
                if category not in matched_categories:
                    matched_categories.append(category)
    
    return matched_categories

def select_questions_by_difficulty(questions: List[Dict], difficulty: str = 'mixed', count: int = 5) -> List[Dict]:
    if difficulty == 'mixed':
        easy_questions = [q for q in questions if q['difficulty'] == 'easy']
        medium_questions = [q for q in questions if q['difficulty'] == 'medium']
        hard_questions = [q for q in questions if q['difficulty'] == 'hard']
        
        import random
        selected = []
        selected.extend(random.sample(easy_questions, min(2, len(easy_questions))))
        selected.extend(random.sample(medium_questions, min(2, len(medium_questions))))
        selected.extend(random.sample(hard_questions, min(1, len(hard_questions))))
        
        if len(selected) < count:
            remaining = count - len(selected)
            all_questions = [q for q in questions if q not in selected]
            selected.extend(random.sample(all_questions, min(remaining, len(all_questions))))
        
        return selected[:count]
    else:
        filtered = [q for q in questions if q['difficulty'] == difficulty]
        import random
        return random.sample(filtered, min(count, len(filtered)))

def generate_interview_questions(analysis: Dict[str, any], questions_file: str = 'interview_questions.json', 
                                 difficulty: str = 'mixed', question_count: int = 5) -> str:
    questions_db = load_interview_questions(questions_file)
    
    if not questions_db['categories']:
        return "## âŒ é¢è¯•é¢˜åº“æœªæ‰¾åˆ°\n\nè¯·ç¡®ä¿ interview_questions.json æ–‡ä»¶å­˜åœ¨ã€‚"
    
    target_position = analysis.get('target_position', '')
    skills = analysis.get('skills', [])
    
    matched_categories = []
    
    if target_position:
        position_categories = match_category_from_position(target_position, questions_db)
        matched_categories.extend(position_categories)
    
    if skills:
        skill_categories = match_category_from_skills(skills, questions_db)
        matched_categories.extend(skill_categories)
    
    matched_categories = list(set(matched_categories))
    
    if not matched_categories:
        matched_categories = ['frontend', 'backend']
    
    all_questions = []
    for category in matched_categories:
        if category in questions_db['categories']:
            all_questions.extend(questions_db['categories'][category]['questions'])
    
    if not all_questions:
        return "## âŒ æœªæ‰¾åˆ°åŒ¹é…çš„é¢è¯•é¢˜\n\nè¯·æ£€æŸ¥é¢˜åº“é…ç½®æˆ–ç®€å†ä¿¡æ¯ã€‚"
    
    selected_questions = select_questions_by_difficulty(all_questions, difficulty, question_count)
    
    output = []
    output.append("## ğŸ“ æ¨èé¢è¯•é¢˜\n")
    
    difficulty_names = {
        'easy': 'åˆçº§',
        'medium': 'ä¸­çº§',
        'hard': 'é«˜çº§'
    }
    
    for i, question in enumerate(selected_questions, 1):
        output.append(f"### é¢˜ç›® {i}")
        output.append(f"**éš¾åº¦**: {difficulty_names.get(question['difficulty'], question['difficulty'])}")
        output.append(f"**é—®é¢˜**: {question['question']}")
        output.append(f"**å‚è€ƒç­”æ¡ˆ**: {question['answer']}")
        output.append(f"**æ ‡ç­¾**: {', '.join(question['tags'])}")
        output.append("")
    
    output.append(f"\n**åŒ¹é…çš„æŠ€èƒ½ç±»åˆ«**: {', '.join(matched_categories)}")
    
    return '\n'.join(output)

with gr.Blocks(title="ç®€å†åˆ†æå™¨") as demo:
    gr.Markdown("# ğŸ“„ ç®€å†åˆ†æå™¨")
    gr.Markdown("ä¸Šä¼ ç®€å†æ–‡ä»¶ï¼ˆPDFæˆ–Markdownï¼‰ï¼Œä½¿ç”¨AIå¤§æ¨¡å‹è‡ªåŠ¨æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆé¢è¯•é¢˜")
    
    with gr.Row():
        with gr.Column():
            file_input = gr.File(
                label="ä¸Šä¼ ç®€å†æ–‡ä»¶",
                file_types=[".pdf", ".md", ".markdown"],
                type="filepath"
            )
            with gr.Row():
                extract_btn = gr.Button("åˆ†æç®€å†", variant="primary")
                clear_btn = gr.Button("æ¸…ç©º", variant="secondary")
    
    with gr.Row():
        with gr.Column():
            analysis_output = gr.Markdown(
                label="åˆ†æç»“æœ",
                value="*åˆ†æç»“æœå°†æ˜¾ç¤ºåœ¨è¿™é‡Œ...*"
            )
    
    gr.Markdown("---")
    gr.Markdown("## ğŸ¯ é¢è¯•é¢˜ç›®ç”Ÿæˆ")
    
    with gr.Row():
        difficulty = gr.Radio(
            choices=["mixed", "easy", "medium", "hard"],
            value="mixed",
            label="é¢˜ç›®éš¾åº¦",
            info="é€‰æ‹©é¢è¯•é¢˜ç›®çš„éš¾åº¦çº§åˆ«"
        )
        question_count = gr.Slider(
            minimum=1,
            maximum=10,
            value=5,
            step=1,
            label="é¢˜ç›®æ•°é‡",
            info="é€‰æ‹©è¦ç”Ÿæˆçš„é¢è¯•é¢˜æ•°é‡"
        )
        generate_questions_btn = gr.Button("ç”Ÿæˆé¢è¯•é¢˜", variant="primary")
    
    interview_questions_output = gr.Markdown(
        label="é¢è¯•é¢˜ç›®",
        value="*åˆ†æç®€å†åï¼Œç‚¹å‡»ç”Ÿæˆé¢è¯•é¢˜æŒ‰é’®...*"
    )
    
    gr.Markdown("""
    ## ä½¿ç”¨è¯´æ˜
    
    1. ç‚¹å‡»ä¸Šä¼ æŒ‰é’®é€‰æ‹©ç®€å†æ–‡ä»¶ï¼ˆæ”¯æŒPDFã€.mdã€.markdownæ ¼å¼ï¼‰
    2. ç‚¹å‡»"åˆ†æç®€å†"æŒ‰é’®ï¼Œç³»ç»Ÿä¼šä½¿ç”¨AIå¤§æ¨¡å‹æ™ºèƒ½åˆ†æç®€å†
       - PDFæ–‡ä»¶ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
       - Markdownæ–‡ä»¶ç›´æ¥è¯»å–å†…å®¹
    3. æŸ¥çœ‹åˆ†æç»“æœ
    4. é€‰æ‹©é¢˜ç›®éš¾åº¦å’Œæ•°é‡
    5. ç‚¹å‡»"ç”Ÿæˆé¢è¯•é¢˜"æŒ‰é’®ï¼Œç³»ç»Ÿä¼šæ ¹æ®ç®€å†ä¸­çš„æŠ€èƒ½å’Œå²—ä½è‡ªåŠ¨åŒ¹é…åˆé€‚çš„é¢è¯•é¢˜
    
    ## åˆ†æåŠŸèƒ½
    
    - **åŸºæœ¬ä¿¡æ¯**: å§“åã€è”ç³»æ–¹å¼
    - **æ±‚èŒæ„å‘**: åº”è˜å²—ä½ã€å·¥ä½œå¹´é™
    - **ä¸“ä¸šæŠ€èƒ½**: AIæ™ºèƒ½è¯†åˆ«æŠ€æœ¯æ ˆå’ŒæŠ€èƒ½
    - **å­¦å†ä¿¡æ¯**: æ•™è‚²èƒŒæ™¯
    - **æ ¸å¿ƒäº®ç‚¹**: AIåˆ†æå€™é€‰äººçš„æ ¸å¿ƒä¼˜åŠ¿
    - **ä¸ªäººç®€ä»‹**: è‡ªæˆ‘è¯„ä»·å’Œç®€ä»‹
    
    ## é¢è¯•é¢˜ç”Ÿæˆ
    
    - æ ¹æ®åº”è˜å²—ä½è‡ªåŠ¨åŒ¹é…æŠ€æœ¯ç±»åˆ«
    - æ ¹æ®æŠ€èƒ½åˆ—è¡¨æ™ºèƒ½é€‰æ‹©ç›¸å…³é¢˜ç›®
    - æ”¯æŒæŒ‰éš¾åº¦ç­›é€‰é¢˜ç›®ï¼ˆåˆçº§/ä¸­çº§/é«˜çº§/æ··åˆï¼‰
    - æä¾›å‚è€ƒç­”æ¡ˆå’Œæ ‡ç­¾
    
    ## æ³¨æ„äº‹é¡¹
    
    - æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: .pdf, .md, .markdown
    - æ–‡ä»¶ç¼–ç å¿…é¡»æ˜¯UTF-8ï¼ˆMarkdownæ–‡ä»¶ï¼‰
    - éœ€è¦é…ç½®é€šä¹‰åƒé—®APIå¯†é’¥ï¼ˆå·²åœ¨.envä¸­é…ç½®ï¼‰
    - PDFè½¬æ¢éœ€è¦å®‰è£… pdfplumber åº“
    - å»ºè®®ä½¿ç”¨æ ‡å‡†ç®€å†æ ¼å¼ä»¥è·å¾—æ›´å¥½çš„åˆ†ææ•ˆæœ
    - é¢è¯•é¢˜åº“ä½äº interview_questions.jsonï¼Œå¯ä»¥è‡ªå®šä¹‰æ·»åŠ é¢˜ç›®
    """)
    
    analysis_state = gr.State(None)
    
    extract_btn.click(
        fn=extract_markdown_content,
        inputs=[file_input],
        outputs=[analysis_output, analysis_state]
    )
    
    clear_btn.click(
        fn=clear_content,
        inputs=[],
        outputs=[analysis_output, analysis_state]
    )
    
    generate_questions_btn.click(
        fn=lambda analysis, diff, count: generate_interview_questions(analysis, difficulty=diff, question_count=count) if analysis else "è¯·å…ˆåˆ†æç®€å†",
        inputs=[analysis_state, difficulty, question_count],
        outputs=[interview_questions_output]
    )

if __name__ == "__main__":
    demo.launch(server_port=7864)
